{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EIlSWxzLSPO",
        "outputId": "25dbe3de-35ff-414d-fa8a-3d458d9cd381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "  !pip install pyspark\n",
        "  !pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Pyspark_2\").getOrCreate()\n",
        "print(spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk3jrOGzMW-l",
        "outputId": "79e448d3-895a-411a-9a5b-2f9c4ac7af3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [(1, 'Jay'), (2, 'Avi'), (3, 'Saint'), (4,'Jerome')]\n",
        "columns = ['ID', 'Name']\n",
        "df1=spark.createDataFrame(data1,columns)\n",
        "df1.show()\n",
        "data2 = [(5,'Mohan'),(6,'Teddy'), (7,'Leon'), (8,'Gustavo')]\n",
        "df2=spark.createDataFrame(data2,columns)\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUQ-8vFLMbl6",
        "outputId": "cc3a3fa5-7e64-48d9-e849-56212214d453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| ID|  Name|\n",
            "+---+------+\n",
            "|  1|   Jay|\n",
            "|  2|   Avi|\n",
            "|  3| Saint|\n",
            "|  4|Jerome|\n",
            "+---+------+\n",
            "\n",
            "+---+-------+\n",
            "| ID|   Name|\n",
            "+---+-------+\n",
            "|  5|  Mohan|\n",
            "|  6|  Teddy|\n",
            "|  7|   Leon|\n",
            "|  8|Gustavo|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "#Union\n",
        "\n",
        "UnionDF=df1.union(df2)\n",
        "UnionDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4ZvQAjNN99F",
        "outputId": "cc8eecc1-517b-4f4e-d987-d26914682dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| ID|   Name|\n",
            "+---+-------+\n",
            "|  1|    Jay|\n",
            "|  2|    Avi|\n",
            "|  3|  Saint|\n",
            "|  4| Jerome|\n",
            "|  5|  Mohan|\n",
            "|  6|  Teddy|\n",
            "|  7|   Leon|\n",
            "|  8|Gustavo|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHZIdfnjyVBo",
        "outputId": "ff7e6d68-aa3a-4f6a-acc7-969554302d33"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|        Name|\n",
            "+-----+------------+\n",
            "|    1|  john jones|\n",
            "|    2|tracey smith|\n",
            "|    3| amy sanders|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "print(rdd)\n",
        "print(\"Initial partition count:\"+str(rdd.getNumPartitions()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbp_giMIGmp5",
        "outputId": "b4ce791f-555c-402c-dff9-13095ffb1a03"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[85] at readRDDFromFile at PythonRDD.scala:289\n",
            "Initial partition count:2\n"
          ]
        }
      ]
    }
  ]
}